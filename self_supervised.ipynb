{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "self_supervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majidsadrii/self-supervised/blob/main/self_supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. *Imports*"
      ],
      "metadata": {
        "id": "r_JEIP44sE6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0sWmbnuXq2pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62560fb7-70f3-46b1-db75-150e051d4f0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "96qEkyyIGbWN"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import math\n",
        "\n",
        "! pip install colorama\n",
        "from colorama import Fore"
      ],
      "metadata": {
        "id": "y8GaUf10GdWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7451499-d17b-46aa-d3e0-179311ad0f24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% cd /content/drive/MyDrive/my_code/25_pso_second_step/cond_with_rotation\n",
        "# % cd /content/drive/MyDrive/cond_with_rotation"
      ],
      "metadata": {
        "id": "Dyb8W7ugsWRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51be2673-4ee2-4b7f-b455-14193d97a6b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/my_code/25_pso_second_step/cond_with_rotation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataloader.dataloader import CIFAR10_modified , CIFAR100_modified\n",
        "from visulization.dataset_vis import rotated_dataset_plot\n",
        "from model.dense import CondenseNet"
      ],
      "metadata": {
        "id": "NyTP0bhRxb4i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Parameters"
      ],
      "metadata": {
        "id": "1Xy0AP_gsDah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reslut_name = 'condens_with_selfsupervised6'\n",
        "\n",
        "##################\n",
        "# training parameter\n",
        "batch_size = 32\n",
        "weight_pr = 1\n",
        "\n",
        "learning_rate = 0.1\n",
        "momentum = 0.9\n",
        "W_decay  = 0.001\n",
        "lr_type = 'cosine'\n",
        "##################\n",
        "\n",
        "\n",
        "unsupervised_training =True\n",
        "main_dataset  = 'Cifar10'\n",
        "pre_text_dataset = 'Cifar10'\n",
        "\n",
        "##############################\n",
        "# CondenseNet parameters\n",
        "model_parm ={}\n",
        "model_parm['stages']  =  [14,14,14]\n",
        "model_parm['data']  = 'cifar10'\n",
        "model_parm['growth'] =[8,16,32]\n",
        "model_parm['num_classes'] = 10\n",
        "model_parm['group_1x1'] = 4\n",
        "model_parm['group_3x3'] = 4\n",
        "model_parm['bottleneck'] = 4\n",
        "model_parm['condense_factor'] =4\n",
        "model_parm['dropout_rate'] = 0\n",
        "###############################\n",
        "\n",
        "##################### saving parameters as a csv file\n",
        "\n",
        "info = model_parm\n",
        "info['batch_size'] = batch_size\n",
        "info['weight_pr '] = weight_pr \n",
        "info['learning_rate']  = learning_rate \n",
        "info['momentum']  =  momentum\n",
        "info['W_decay']  =  W_decay\n",
        "\n",
        "df = pd.DataFrame(info)\n",
        "df_name = '{}.csv'.format(reslut_name)\n",
        "df.to_csv(df_name)"
      ],
      "metadata": {
        "id": "tJQ80hd5wlIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TRSyxw48ijq",
        "outputId": "a91a3dd4-68c5-4769-f5b1-4162c33ab891"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " best_condens_without_ss        condens_with_selfsupervised2.pt\n",
            " best_condens_without_ss2       condens_with_selfsupervised3\n",
            " best_condens_without_ss3       condens_with_selfsupervised3.pt\n",
            " best_condens_without_ss4       condens_with_selfsupervised4\n",
            " best_condens_without_ss5       condens_with_selfsupervised4.pt\n",
            " ch_condens_without_ss2.pt      condens_with_selfsupervised5\n",
            " ch_condens_without_ss3.pt      condens_with_selfsupervised5.pt\n",
            " ch_condens_without_ss4.pt      condens_with_selfsupervised.csv\n",
            " ch_condens_without_ss5.pt      condens_with_selfsupervised.png\n",
            " ch_condens_without_ss.pt       condens_with_selfsupervised.pt\n",
            " condens_without_ss2.csv        cond_model_without_selfsupervised.ipynb\n",
            " condens_without_ss2.png       'Copy of cond2.ipynb'\n",
            " condens_without_ss3.csv        data\n",
            " condens_without_ss3.png        dataloader\n",
            " condens_without_ss4.csv        main.ipynb\n",
            " condens_without_ss4.png        model\n",
            " condens_without_ss5.csv       'result '\n",
            " condens_without_ss5.png        runs20220726_072145\n",
            " condens_without_ss.csv         runs20220726_074135\n",
            " condens_without_ss.png         runs20220726_082045\n",
            " condens_without_ss.pt\t        runs20220726_082110\n",
            " condens_with_selfsupervised    self_supervised.ipynb\n",
            " condens_with_selfsupervised2   visulization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(    [transforms.ToTensor(),transforms.Normalize(mean=[0.4914, 0.4824, 0.4467], std=[0.2471, 0.2435, 0.2616])])\n",
        "if main_dataset  == 'Cifar10':\n",
        "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                          download=True, transform=transform)\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                            shuffle=True)\n",
        "  testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "  classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "EcsX6hHdGh8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if unsupervised_training:\n",
        "  if pre_text_dataset == 'Cifar100':\n",
        "    pretext_data = CIFAR100_modified(root='./data', train=True,download=True, transform=transform)\n",
        "    pretext_data_loader = torch.utils.data.DataLoader(pretext_data, batch_size=batch_size, shuffle=True)\n",
        "  elif  pre_text_dataset == 'Cifar10':\n",
        "    pretext_data = CIFAR10_modified(root='./data', train=True,download=True, transform=transform)\n",
        "    pretext_data_loader = torch.utils.data.DataLoader(pretext_data, batch_size=batch_size, shuffle=True)\n",
        "# plotting some sample\n",
        "  rotated_dataset_plot(pretext_data_loader)"
      ],
      "metadata": {
        "id": "w3vmErZA9dOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Model"
      ],
      "metadata": {
        "id": "P_poIHHC8hOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = CondenseNet( model_parm['stages'] ,model_parm['data'], model_parm['growth'],model_parm['num_classes'],model_parm['group_1x1'],model_parm['group_3x3'],model_parm['bottleneck'],model_parm['condense_factor'],model_parm['dropout_rate'] )\n"
      ],
      "metadata": {
        "id": "gILYdpZqDQzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.to(device)"
      ],
      "metadata": {
        "id": "A2KckQSDDQ1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model training"
      ],
      "metadata": {
        "id": "pqt1glch90Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(my_model.parameters(), lr=0.0001)\n",
        "# optimizer = optim.SGD(my_model.parameters(), lr=learning_rate, momentum=moment,weight_decay=W_decay)\n",
        "optimizer = torch.optim.SGD(my_model.parameters(), lr=learning_rate,\n",
        "                                momentum=momentum,\n",
        "                                weight_decay=W_decay,\n",
        "                                nesterov=True)"
      ],
      "metadata": {
        "id": "E1EkjksOHTrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, EPOCHS,learning_rate ,batch=None,\n",
        "                         nBatch=None, method='cosine'):\n",
        "    if method == 'cosine':\n",
        "        T_total = EPOCHS  * nBatch\n",
        "        T_cur = (epoch % EPOCHS ) * nBatch + batch\n",
        "        lr = 0.5 * learning_rate * (1 + math.cos(math.pi * T_cur / T_total))\n",
        "    elif method == 'multistep':\n",
        "       \n",
        "            lr, decay_rate = learning_rate, 0.1\n",
        "            if epoch >= EPOCHS * 0.75:\n",
        "                lr *= decay_rate**2\n",
        "\n",
        "            elif epoch >= EPOCHS * 0.5:\n",
        "                lr *= decay_rate\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr"
      ],
      "metadata": {
        "id": "hyOpMH7xuDMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch, tb_writer, total_bach,lr_type,optimizer,EPOCHS,learning_rate ):\n",
        "\n",
        "    running_loss_1 = 0.\n",
        "    last_loss_1 = 0.\n",
        "    running_loss_2 = 0.\n",
        "    last_loss_2 = 0.\n",
        "    acc = 0\n",
        "    tot = 0\n",
        "    cor = 0\n",
        "    acc2 = 0\n",
        "    tot2 = 0\n",
        "    cor2 = 0\n",
        "    training_loss = 0\n",
        "    running_lr = None\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(trainloader,0):\n",
        "\n",
        "        lr = adjust_learning_rate(optimizer, epoch,EPOCHS = EPOCHS,learning_rate = learning_rate, batch=i,\n",
        "                                  nBatch=total_bach, method=lr_type )\n",
        "        if running_lr is None:\n",
        "            running_lr = lr\n",
        "\n",
        "\n",
        "      # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()     \n",
        "        image,label = data\n",
        "        image,label = image.to(device),label.to(device)\n",
        "        # Make predictions for this batch\n",
        "        outputs,_ = my_model(image)            \n",
        "        loss_1 = criterion(outputs,label) \n",
        "        running_loss_1 += loss_1.item()        \n",
        "        _, pr= torch.max( outputs.data, 1)\n",
        " \n",
        "        # _, real= torch.max(label.data, 1)\n",
        "\n",
        "        tot += label.size(0)\n",
        "        cor += (pr ==  label).sum().item()\n",
        "        acc = 100 * cor / tot    \n",
        "        \n",
        "        if unsupervised_training:\n",
        "          _,image_r,rotation = next(iter(pretext_data_loader))            \n",
        "          image_r,rotation =  image_r.to(device),rotation.to(device)      \n",
        "          _,outputs_r = my_model( image_r) \n",
        "          loss_2 = criterion(outputs_r,rotation)\n",
        "          running_loss_2 += loss_2.item()\n",
        "          loss =  loss_1 + weight_pr *loss_2            \n",
        "          _, pr2= torch.max(outputs_r.data, 1)\n",
        "          _, real2= torch.max(rotation.data, 1)\n",
        "          tot2 += rotation.size(0)\n",
        "          cor2 += (pr2 == real2).sum().item()\n",
        "          acc2 = 100 * cor2 / tot2\n",
        "        else:\n",
        "          loss =  loss_1\n",
        "\n",
        "        loss.backward()      \n",
        "        optimizer.step()      \n",
        "\n",
        "        last_loss_1 += loss_1 \n",
        "        last_loss_2 += loss_2\n",
        "        if i % 50 == 49:\n",
        "\n",
        "          R1 = running_loss_1/50\n",
        "          R2 =running_loss_2/50\n",
        "          print(Fore.BLUE,'{}/ {} train_l1: {} train_l2: {}, accu is {}  pretex_accu is {} time is {}'.format(i + 1,total_bach, '%.5f' %(R1),'%.5f' %(R2),'%.4f' %acc,'%.4f' %acc2,'%.5f' %(time.time() - start_time)))\n",
        "          \n",
        "          running_loss_1 = 0.\n",
        "          running_loss_2 = 0. \n",
        "\n",
        "        \n",
        "    PATH ='{}.pt'.format(reslut_name)\n",
        "    \n",
        "    torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': my_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, PATH)\n",
        "\n",
        "    return (last_loss_1/total_bach),(last_loss_2/total_bach),acc,acc2,running_lr"
      ],
      "metadata": {
        "id": "jdErXrUr9m5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_result(train_loss_main,test_loss_main,train_accuracy_main,test_accuracy_main):\n",
        "    E = [i for i in range(len(train_loss_main))]\n",
        "\n",
        "    fig, (axs, ax2,ax3) = plt.subplots(3)\n",
        "    t_train_loss_main = [train_loss_main[i].cpu().detach().numpy()  for i  in range(len(train_loss_main))]\n",
        "    s_test_loss_main =  [test_loss_main[i].cpu().detach().numpy()  for i  in range(len(test_loss_main))]\n",
        "    axs.plot(E, t_train_loss_main)\n",
        "    axs.plot(E,s_test_loss_main)    \n",
        "    axs.set_xlabel('epoch')\n",
        "    axs.set_ylabel('loss')\n",
        "    axs.grid(True)\n",
        "\n",
        "    t_train_accuracy_main  = [train_accuracy_main [i]  for i  in range(len(train_accuracy_main ))]\n",
        "    s_test_accuracy_main =  [test_accuracy_main[i] for i  in range(len(test_accuracy_main))]\n",
        "    ax2.plot(E, t_train_accuracy_main )\n",
        "    ax2.plot(E,s_test_accuracy_main)    \n",
        "    ax2.set_xlabel('epoch')\n",
        "    ax2.set_ylabel('accuracy')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    t_train_accuracy_pre = [train_accuracy_pre[i]  for i  in range(len(train_loss_main))]\n",
        "    ax3.plot(E, t_train_accuracy_pre )\n",
        "    ax3.set_xlabel('epoch')\n",
        "    ax3.set_ylabel('pretext_accuracy')\n",
        "\n",
        "    fig.tight_layout()\n",
        "    image_name =reslut_name +'.png'\n",
        "    plt.savefig(image_name )\n",
        "\n",
        "    dict = {'acc_train': t_train_accuracy_main, \n",
        "            'acc_test': s_test_accuracy_main,\n",
        "            'loss_train': t_train_loss_main,\n",
        "            'loss_test':s_test_loss_main}\n",
        "    df = pd.DataFrame(dict)\n",
        "    df_name = '{}.csv'.format(reslut_name)\n",
        "    df.to_csv(df_name)"
      ],
      "metadata": {
        "id": "DaOlyLqIp1-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "total_bach = len(trainloader)\n",
        "total_batch_test = len(testloader)\n",
        "writer = SummaryWriter('runs{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "EPOCHS = 200\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "train_loss_main =[]\n",
        "train_accuracy_main =[]\n",
        "\n",
        "train_loss_pre =[]\n",
        "train_accuracy_pre =[]\n",
        "\n",
        "test_loss_main =[]\n",
        "test_accuracy_main =[]\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    my_model.train(True) \n",
        "   \n",
        "\n",
        "    L1, L2,acc,acc2,lr= train_one_epoch(epoch_number, writer,total_bach,lr_type,optimizer,EPOCHS,learning_rate)\n",
        "    train_loss_main.append(L1)    \n",
        "    train_accuracy_main.append(acc)\n",
        "    if unsupervised_training:\n",
        "      train_loss_pre.append(L2)\n",
        "      train_accuracy_pre.append(acc2)\n",
        " \n",
        "    running_vloss = 0.0\n",
        "    correct =0\n",
        "    total = 0.\n",
        "    my_model.train(False) \n",
        "    for i, vdata in enumerate(testloader ):\n",
        "        with torch.no_grad():\n",
        "          v_image, vlabels = vdata\n",
        "          v_image, vlabels =  v_image.to(device), vlabels.to(device)\n",
        "          voutputs,_  =  my_model(v_image)          \n",
        "                      \n",
        "          vloss = criterion(voutputs, vlabels)\n",
        "          running_vloss += vloss\n",
        "\n",
        "          _, predicted = torch.max(voutputs.data, 1)\n",
        "          # _, real_label = torch.max(vlabels.data, 1)\n",
        "          total += vlabels.size(0)\n",
        "          correct += (predicted == vlabels.data).sum().item()\n",
        "         \n",
        "    accuracy = 100 * correct / total\n",
        "    avg_vloss = running_vloss / (total_batch_test)\n",
        "    test_loss_main.append(avg_vloss)\n",
        "    test_accuracy_main.append(accuracy)\n",
        "\n",
        "\n",
        "    print(Fore.GREEN,'for main taks,loss_test {},..... acc_test {}'.format(avg_vloss, accuracy))   \n",
        "  \n",
        "\n",
        "    if i %5 ==0:\n",
        "          plot_result(train_loss_main,test_loss_main,train_accuracy_main,test_accuracy_main)\n",
        "\n",
        "       # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = '{}'.format(reslut_name)\n",
        "        torch.save(my_model.state_dict(),model_path)\n",
        "\n",
        "\n",
        "  \n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "id": "8PUmhOfbAC3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86cd115f-c0cf-4935-82b2-86748f0ff9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m 750/ 1563 train_l1: 0.33488 train_l2: 0.95210, accu is 88.1333  pretex_accu is 78.2375 time is 3141.16201\n",
            "\u001b[34m 800/ 1563 train_l1: 0.33739 train_l2: 0.96363, accu is 88.1289  pretex_accu is 78.1914 time is 3153.91006\n",
            "\u001b[34m 850/ 1563 train_l1: 0.39246 train_l2: 0.96715, accu is 88.0037  pretex_accu is 78.1397 time is 3166.58829\n",
            "\u001b[34m 900/ 1563 train_l1: 0.39316 train_l2: 0.95330, accu is 87.8958  pretex_accu is 78.1667 time is 3179.44159\n",
            "\u001b[34m 950/ 1563 train_l1: 0.36345 train_l2: 0.96528, accu is 87.8388  pretex_accu is 78.1283 time is 3192.22458\n",
            "\u001b[34m 1000/ 1563 train_l1: 0.37789 train_l2: 0.95380, accu is 87.7500  pretex_accu is 78.1594 time is 3205.02462\n",
            "\u001b[34m 1050/ 1563 train_l1: 0.37154 train_l2: 0.94549, accu is 87.6905  pretex_accu is 78.2083 time is 3217.90110\n",
            "\u001b[34m 1100/ 1563 train_l1: 0.34595 train_l2: 0.95879, accu is 87.7102  pretex_accu is 78.2244 time is 3230.64826\n",
            "\u001b[34m 1150/ 1563 train_l1: 0.34548 train_l2: 0.97098, accu is 87.7174  pretex_accu is 78.1658 time is 3243.64806\n",
            "\u001b[34m 1200/ 1563 train_l1: 0.35895 train_l2: 0.95095, accu is 87.7292  pretex_accu is 78.1901 time is 3256.39925\n",
            "\u001b[34m 1250/ 1563 train_l1: 0.39061 train_l2: 0.95733, accu is 87.6450  pretex_accu is 78.2025 time is 3269.15728\n",
            "\u001b[34m 1300/ 1563 train_l1: 0.42669 train_l2: 0.96033, accu is 87.5529  pretex_accu is 78.1827 time is 3281.85603\n",
            "\u001b[34m 1350/ 1563 train_l1: 0.37582 train_l2: 0.96604, accu is 87.5139  pretex_accu is 78.1667 time is 3294.59667\n",
            "\u001b[34m 1400/ 1563 train_l1: 0.35481 train_l2: 0.94840, accu is 87.4888  pretex_accu is 78.1964 time is 3307.39033\n",
            "\u001b[34m 1450/ 1563 train_l1: 0.35380 train_l2: 0.94253, accu is 87.5086  pretex_accu is 78.2500 time is 3320.18272\n",
            "\u001b[34m 1500/ 1563 train_l1: 0.35841 train_l2: 0.94953, accu is 87.5125  pretex_accu is 78.2854 time is 3332.89069\n",
            "\u001b[34m 1550/ 1563 train_l1: 0.34510 train_l2: 0.95656, accu is 87.5081  pretex_accu is 78.2863 time is 3345.62158\n",
            "\u001b[32m for main taks,loss_test 0.45410096645355225,..... acc_test 85.02\n",
            "EPOCH 9:\n",
            "\u001b[34m 50/ 1563 train_l1: 0.28802 train_l2: 0.95697, accu is 90.6875  pretex_accu is 78.0625 time is 3383.16094\n",
            "\u001b[34m 100/ 1563 train_l1: 0.30774 train_l2: 0.94462, accu is 90.0938  pretex_accu is 78.7812 time is 3395.85765\n",
            "\u001b[34m 150/ 1563 train_l1: 0.29023 train_l2: 0.95224, accu is 90.0417  pretex_accu is 78.7083 time is 3408.54643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D2SMVVvyBiUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z4nt4W-S1HEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}